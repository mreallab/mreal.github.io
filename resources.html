<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MReaL Lab</title>
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <div id="mobile-menu-open" class="shadow-large">
        <i class="fa fa-bars" aria-hidden="true"></i>
    </div>
    <!-- End #mobile-menu-toggle -->
    <header>
        <div class="header">
            <div class="container">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="#" class="logo"><img src="images/logo.png" height="30px" alt="" /></a></li>
                    <li><a onclick="window.location='./index.html'"> News </a></li>
                    <li><a onclick="window.location='./people.html'"> People </a></li>
                    <li><a onclick="window.location='./publications.html'"> Publications </a></li>
                    <li><a onclick="window.location='./resources.html'"> Resources </a></li>
                    <li><a onclick="window.location='./index.html#contact'"> Contact </a></li>
                </ul>
            </div>
        </div>
    </header>
    <!-- End header -->

    <div id="projects" class="background-alt">
        <h2 class="heading"><br><b>Seminar</b></h2>
        <div class="container">
            <div class="row">


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=180 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Sharing on Reinforcement Learning
                            <br> Speaker: Mingjie Lao, April 24 2019</h4>
                        <p>
                            <b>Abstract</b>: The objective of Reinforcement learning is to find the optimal policy that
                            maximizes rewards in the long run. In this talk I will talk about 3 types of RL algorithms:
                            1. Policy gradient; 2. Actor-Critic; 3. Q-learning.
                            Concepts will be explained with illustration and papers from OpenAI will be shared.

                        </p>
                        <a href="https://drive.google.com/file/d/1YIb7ykCYLvxe7gpnyHbsY6QTgYNoLSlA/view">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->



                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=200 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Deep Video Feature Learning
                            <br> Speaker: Ruiwei Zhao, April 17 2019</h4>
                        <p>
                            <b>Abstract</b>: Video feature learning for action recognition is a challenging task that
                            has been extensively studied in the research community. How to properly exploit the motion
                            and temporal information are key to the design of the models. In this talk, I will review
                            some famous CNN/LSTM based networks designed for action recognition, including multi-stream
                            CNNs, 3D convolution with its variants, and non-local neural networks, etc.

                        </p>
                        <a href="https://drive.google.com/file/d/1-8OrpV0SDLdn5O2EZXcAj8rEFyqbA9Fp/view">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=230 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Image Captioning
                            <br> Speaker: Xu Yang, April 3 2019</h4>
                        <p>
                            <b>Abstract</b>: The target of image captioning is to generate a syntactically and
                            semantically correct sentence which can describe the main content of the given image.
                            Compared with early image captioners which are rules/templates based, the modern captioning
                            models have achieved striking advances by three key techniques, i.e., encoder-decoder based
                            pipeline, attention technique, and RL-based training objective. However, these image
                            captioners lack the ability of commonsense reasoning, which is one important inductivce bias
                            owned by humans. For exploiting such language inductive bias, Scene Graph Auto-Encoder
                            (SGAE) is proposed for generating more descriptive captions.

                        </p>
                        <a href="https://drive.google.com/open?id=1ZDBfzetkb6CS1_EQSrwhxSd12iRmsNrN">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=300 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Recursive Visual Attention in Visual Dialog
                            <br> Speaker: Yulei Niu, March 27 2019</h4>
                        <p>
                            <b>Abstract</b>: Visual dialog is a challenging vision-language task, which requires the
                            agent to answer multi-round questions about an image. It typically needs to address two
                            major problems: (1) How to answer visually-grounded questions, which is the core challenge
                            in visual question answering (VQA); (2) How to infer the co-reference between questions and
                            the dialog history. An example of visual co-reference is: pronouns (e.g., `they') in the
                            question (e.g., `Are they on or off?') are linked with nouns (e.g., `lamps') appearing in
                            the dialog history (e.g., `How many lamps are there?') and the object grounded in the image.
                            In this work, to resolve the visual co-reference for visual dialog, we propose a novel
                            attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent
                            browses the dialog history until the agent has sufficient confidence in the visual
                            co-reference resolution, and refines the visual attention recursively. The quantitative and
                            qualitative experimental results on the large-scale VisDial v0.9 and v1.0 datasets
                            demonstrate that the proposed RvA not only outperforms the state-of-the-art methods, but
                            also achieves reasonable recursion and interpretable attention maps without additional
                            annotations.

                        </p>
                        <a href="https://drive.google.com/open?id=1LiIjun3zfUsuaSrmmRbbGRTIkmJCtAWG">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=195 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Instance Segmentation
                            <br> Speaker: Xinting Hu, March 13 2019</h4>
                        <p>
                            <b>Abstract</b>: Instance segmentation is a long-lasting problem in computer vision and a
                            basic component of many applications, such as autonomous driving. Current instance
                            segmentation methods based on deep neural networks can be categorized into two types,
                            depending on how the method approaches the problem by starting from either detection or
                            segmentation modules. In this presentation, I will give introduction of thses two kinds of
                            methods and futhur cover some ideas about panoptic segmentation.

                        </p>
                        <a href="https://drive.google.com/file/d/1cUXhSV2jY6-KWBPX6u31uaGtwrGuoy4x/view?usp=sharing">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->



                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=195 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Visual Relationship Detection (VRD)
                            <br> Speaker: Mitra, March 06 2019</h4>
                        <p>
                            <b>Abstract</b>: Visual relationships represent the visible and detectable various
                            interactions between each object pair. Also, reasoning those relationships- formalized as
                            Visual Relation Detection (VRD) task- can be fed into higher level tasks such as image
                            captioning, visual question answering, image-text matching, as the intermediate building
                            block.
                            Reviewing of the task’s challenges, available datasets, and some state-of-the-arts is the
                            purpose of seminar ahead.

                        </p>
                        <a href="https://drive.google.com/open?id=1sa-UUTmemtWP_JzBBJPn-MSiDIY16teE">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=195 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: A Closer Look at CNN Architecture Design for Image Recognition
                            <br> Speaker: Long Chen, Feb 27 2019</h4>
                        <p>
                            <b>Abstract</b>: The appearance of AlexNet relight the popularity of deep learning, and many
                            tasks in computer vision had obtained significant improvements based on these basic CNN
                            architectures pretrained on ImageNet. However, even now in 2019, many researchers still only
                            familiar with the ResNet (the winner of 2015 ILSVRC challenges). In this presentation, I
                            will review some famous CNN architectures and analysis the philosophy of these designs, to
                            help design better CNN architecture in our own specific tasks.
                        </p>
                        <a href="https://drive.google.com/open?id=1TyzCHT6_FyNJi6FzU8EsZ-Of4g-pYZGh">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=190 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Visual Question Answering
                            <br> Speaker: Kaihua Tang, Feb 20 2019</h4>
                        <p>
                            <b>Abstract</b>: Visual Question Answering is an important step from low-level cognition
                            tasks, like visual recognition/detection, sentence analysis, towards general artificial
                            intelligence. Although current VQA system is still not perfect, it motivates the community
                            to think about how to build a bridge between visual information and textual information,
                            which are two most important source of information the human can absorb.
                        </p>
                        <a href="https://drive.google.com/file/d/1EENMRiSGZRnnLLo7xChvRPXmG0RDGSwG/view?usp=sharing">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=180 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Visual Dialog
                            <br> Speaker: Jiaxin Qi, Feb 13 2019</h4>
                        <p>
                            <b>Abstract</b>: Visual Dialog is one of the prototype tasks introduced in recent years,
                            which can be viewed as the multi-round VQA. It aims to give a proper answer based on visual
                            and textual contents in the dialog. In this seminar, I will give a brief introduction of its
                            definition, dataset, metrics and methods.
                        </p>
                        <a href="https://drive.google.com/file/d/1Lnkao2GpxrVwKL9pQYah3LzltJtZRpOw/view?usp=sharing">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->



                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=200 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Semantic Image Segmentation
                            <br> Speaker: Dong Zhang, Jan 30 2019</h4>
                        <p>
                            <b>Abstract</b>: The semantic image segmentation is the most elaborate one of three core
                            tasks in computer vision, which aims to assign a correct semantic label to every pixel in an
                            image. In this seminar, I will explain the definition of semantic segmentation and introduce
                            the corresponding datasets. Besides, I will make an analysis and summary to mainstream image
                            semantic segmentation models. At last, I will report to you on my progress in this task.
                            Your comments and criticism are greatly welcomed.
                        </p>
                        <a href="https://drive.google.com/file/d/1i14pecgThwm7ag2QQ4IaB3olZt_UdB7N/view?usp=sharing">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=190 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: An Introduction to Visual Grounding
                            <br>Speaker: Daqing Liu, Jan 23 2019
                        </h4>
                        <p>
                            <b>Abstract</b>: Visual grounding is a task to localize an object in an image based on a
                            query in nature language. It has attracted a lot of attention in the recent years. In this
                            seminar, I'll introduce visual grounding including a) the task definition, b) the datasets,
                            c) a series of papers in mainstream, and d) my recent works. Welcome to join in and please
                            feel free to ask any questions.
                        </p>
                        <a href="https://drive.google.com/file/d/1qe7-0Y-au-hc5zJFolyR_9VSmx4Ee94q/view?usp=sharing">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=160 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>Title: Visual Reasoning
                            <br> Speaker: Jiaxin Shi, Jan 16 2019</h4>
                        <p>
                            <b>Abstract</b>: Visual reasoning aims to answer questions about complicated interactions
                            between
                            visual objects. Existing models can be divided into two categories: holistic approaches and
                            module approaches. I will introduce typical works of these two categories.
                        </p>
                        <a href="https://drive.google.com/file/d/1cDEHZtItzV71W-9gmPp_XicS_MdGSblH/view?usp=sharing">View
                            Slides</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


            </div>
        </div>
    </div>
    <!-- End #projects -->



    <div id="projects" class="background-alt">
        <h2 class="heading"><b>Other Resources</b></h2>
        <div class="container">
            <div class="row">

                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=180 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>The code of paper "Auto-Encoding Scene Graphs for Image Captioning, CVPR 2019" </h4>
                        <p>
                            The code is implemented based on Ruotian Luo's implementation of image captioning in
                            https://github.com/ruotianluo/self-critical.pytorch. And we use the visual features provided
                            by paper Bottom-up and top-down attention for image captioning and visual question answering
                            in https://github.com/peteanderson80/bottom-up-attention. If you like this code, please
                            consider to cite their corresponding papers and my CVPR paper.
                        </p>
                        <a href="https://github.com/yangxuntu/SGAE">[View Github]</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


                <div class="project shadow-large">
                    <div class="project-image">
                        <img src="images/project.jpg" height=160 width=0 />
                    </div>
                    <!-- End .project-image -->
                    <div class="project-info">
                        <h4>The code of paper "Learning to Compose Dynamic Tree Structures for Visual Contexts, CVPR
                            2019" </h4>
                        <p>
                            The code is implemented by pytorch. And we separate the visual question answering and scene
                            graph generation into two repositories in github. The VQA code is directly modified from the
                            project Cyanogenoid/vqa-counting. The SGG code is directly modified from the project
                            rowanz/neural-motifs. If you like this work, cite their corresponding papers and my CVPR
                            paper
                        </p>
                        <a href="https://github.com/KaihuaTang/VCTree-Visual-Question-Answering">[View VQA Code]</a>
                        <a href="https://github.com/KaihuaTang/VCTree-Scene-Graph-Generation">[View SGG Code]</a>
                    </div>
                    <!-- End .project-info -->
                </div>
                <!-- End .project -->


            </div>
        </div>
    </div>
    <!-- End #projects -->

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-sm-5 copyright">
                    <p>
                        Copyright &copy; 2018 Machine Reasoning and Learning Lab
                    </p>
                </div>
                <div class="col-sm-2 top">
                    <span id="to-top">
                        <i class="fa fa-chevron-up" aria-hidden="true"></i>
                    </span>
                </div>
                <div class="col-sm-5 social">
                    <ul>
                        <li>
                            <a href="https://github.com/" target="_blank"><i class="fa fa-github"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://stackoverflow.com/" target="_blank"><i class="fa fa-stack-overflow"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://www.facebook.com/" target="_blank"><i class="fa fa-facebook"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://plus.google.com/" target="_blank"><i class="fa fa-google-plus"
                                    aria-hidden="true"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </footer>
    <!-- End footer -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/scripts.min.js"></script>
</body>

</html>